# 操作系统实验报告

##### 201840058 蒋潇鹏

## L0 hello-os

#### 实现库函数

在ICS中我已经实现了全部的库函数，这里我将他们直接移植过来了。

在stdio.c中我发现函数对共享内存进行了操作，并且printf对putch的调用不能保证输出的原子性，因此我在klib中实现了一个简单的自旋锁，并在这两个地方添加

#### 访问IO设备

这部分我直接魔改了L0的框架代码，把splash函数的调用移到了while(1)中，之后遍历每一个像素点，进行打印。

对于键盘输入，也可以将print_key中的操作魔改一下，当读取到Escape时，调用halt(0)

#### 绘制一个图片

首先我找了一张图片：
![](/home/birdium/os-workbench/kernel/mrs.png)

之后用如下命令将其转换为一个rgb阵列：
```shell
convert mrs.png mrs.rgb
xxd -i mrs.rgb mrs.c
```

这样在`mrs.c`文件中就有了我需要的一个大数组，在splash中用如下代码打印：
```C
  for (int x = 0; x < w; x ++) {
    for (int y = 0; y < h; y++) {
      int loc = (y * IMG_HEIGHT / h) * IMG_WIDTH + (x * IMG_WIDTH / w); 
      unsigned int col = (mrs_rgb[3 * loc] << 16) + 
                         (mrs_rgb[3 * loc + 1] << 8) +
                         (mrs_rgb[3 * loc + 2]);
      draw_tile(x, y, 1, 1, col);
    }
  }
```

## L1 pmm

#### 架构设计

我的设计参考了buddy system和slab的做法，以及mimalloc的部分思想

1. 首先对于堆区的划分：我将堆区的每一页映射到了一个数组 `TableEntry table[MAX_HEAP_SIZE]`, 这个数组起始地址为heap.start, 每个TableEntry维护其对应页面的一些信息，比如大小（2的多少幂次），是否可用，被哪个cpu所分配等各种信息。

2. 对于大内存的分配（>=2page），使用buddy system分配。buddy system维护数个链表，分别存放8KiB, 16KiB, ..., 4MiB的内存项。当进行分配的时候，从恰好满足的大小对应链表开始，对链表上锁，找一片可用内存，将其从链表中移除。如果没有可用内存，就去寻找更大的大小对应的链表。在移除时，在`table`数组中记录它的大小，并将其标记为已分配。当进行释放时，找到table中对应Entry中记录的该内存地址对应区间的大小，将其加入对应链表。加入后检查是否存在可以合并的区间，如果有则合并。

3. 对于小内存的分配，使用slab。slab为每个CPU维护两份缓存链表，一份是local链表，包含了自己申请自己释放的内存空间。一份是thread链表，包含了自己申请其他CPU释放的内存空间。（参考了mimalloc）。在分配时，优先选择local链表。如果local链表不满足，则尝试获取thread链表的锁。依然不满足，则使用buddy system申请大小为4页的内存空间，平均分成数分插入到当前链表之中。在内存分配的时候，保证了一个page上的所有内存都只可能被一个CPU所分配，因此可以在`table`数组中记录这一点。对于内存的释放，判断是哪个CPU分配的，如果分配和释放的是相同CPU，将其插入local list中。否则获取分配CPU对应thread链表的锁，插入thread list之中。

#### 精巧实现

1. 对于页信息使用了`table`来存储，这样就可以用一个相对较小的内存占用来很方便地记录页的meta data。

2. 对于slab中的小区间将信息直接存在对应地址空间中，通过保证每个小区间对应的page只会被一个CPU分配，使得meta data能直接存在`table`数组中

3. 完成了TEST框架的构建，实现了不同等级的日志，能够很方便地进行测试。在这次实验中，thread sanitizer 为我排查了很多并发bug，大大增加了调试效率。

#### 印象深刻的bug

1. 遇到了许多并发bug，首先是因为我buddy system的设计，我是直接用table数组来表示区间的，把区间长度存在table数组里，想要获取其对应的链表，需要先获取长度，因此获取长度的时候没法给链表加锁，导致了这一块可能出现数据竞争。最后用了给table数组加锁的方式来解决。如果重新设计，我会选择用一个类似线段树的方法去表示区间，而不是把表示区间的信息混合在table数组中。

## L2 kmt

### 架构设计

主要实现了`spinlock`, `semaphore`, `kmt` 三个部分.

1. spinlock 部分实现了支持中断嵌套的自旋锁。

2. sem 部分实现了带睡眠的信号量。

3. kmt 部分实现了 `kmt_create, kmt_teardown, kmt_init` 三个接口与两个重要函数 `kmt_context_save` 和 `kmt_schedule` 来支持上下文保存与内核调度。

4. os 部分实现了 `os_trap` 与 `os_irq`， 分别是系统中唯一中断/系统调用的入口和中断处理程序的回调注册


### 精巧实现

1. task_t 的原型定义为：

   ```C++
   typedef struct task {
     enum {SLEEPING, RUNNABLE} status; // 当前task是睡眠/可调度状态
     int running; // 当前task是否正在运行
     const char *name; 
     Context *context; // task保存的上下文
     uint8_t *stack; // task的专属堆栈
     int canary; // 金丝雀
   } task_t;
   ```

2. 自旋锁主要参考了xv6的实现，使用了`push_off`, `pop_off` 两个函数来支持中断（允许嵌套）的保存。

3. 信号量的实现是为信号量维护一个FIFO的等待唤醒链表。

   1. 对于 `sem_wait`，首先获取锁，减小计数器，若计数器为负，将当前task加入链表末尾，并设置其状态为sleeping，之后释放锁并调用yield。
   2. 对于`sem_signal`，首先获取锁，增加计数器，若链表不为空，取链表头的task，将其状态设置为runnable。

4. kmt的实现：

   1. `kmt_init`为每个线程初始化idle线程：设置状态，分配线程栈。同时，`kmt_init`还会注册上下文保存处理函数和调度函数。

   2. `kmt_create` 创建线程并初始化，将其加入全局`task_list`数组。`task_list` 数组维护了所有的非idle的task指针。除此之外这里的`running`被设置成0，而不是init函数中的1

   3. `kmt_teardown` 释放线程栈，并将task从全局数组中移除。

   4. `kmt_context_save` 保存上下文，并检查last数组（具体实现和用处在印象深刻的bug处会讲到）

   5. `kmt_schedule` 进行若干次随机调度线程的尝试——从`task_list` 中随机选择task，检查是否满足条件：

      ```C++
      static const int round = 2; // choose task_cnt times for X round
      for (int i = 0; i < task_cnt * round; i++) {
      	int idx = rand() % task_cnt;
      	task_t *task = task_list[idx];
      	if (task->status != SLEEPING && (task == cur_task || atomic_xchg(&task->running, 1) == 0)) { // atomic set task running 
      		result = task;
      		break;
      	}
      }
      ```

      首先被选中的线程必须睡眠，然后如果选中了不同于陷入trap的线程，需要保证其running不为1——线程可能被其他CPU占据。这里使用了原子操作来保证running读写的原子性，从而不会发生两个CPU同时调度了一个task的状态。

### 印象深刻的bug

1. 首先是在调单线程版本时遇到的一个bug：CPU被调度到神秘的地方，从而引发重启。首先我打印log来debug，发现在某些地方堆栈上存储的rip值被改写了，但是无法定位精确位置。为此我仿照xv6的方法配置了qemu的gdb，在gdb中发现是因为调用printf的原因。我在printf函数中在堆栈上分配了4096B的缓冲区，而这一大小恰好是我为线程分配的堆栈大小。因此一个线程会把另一个线程的栈破坏。

2. 在实现完单核，来考虑多核的时候，会发现有的时候CPU会调度到同一个task。这里我一开始的调度实现是维护一个RUNNABLE的task链表。但这样就遇到了严重问题——信号量和kmt的调度都会修改这个链表，从而导致并发bug。因此我换成了现在的全局数组实现版本。

3. 解决了上面的问题后还存在栈竞争。比如说在多核下，CPU0将context保存到被中断进程A时，还没来得及切换栈，此时另一个CPU1调度了进程A，就造成了两个CPU共用1个栈。此时就无法确定栈上的行为了。为此我为每个CPU维护了一个cur_last来维护这次被中断进程的前一个被中断进程。只有在第二次调度到这个线程之后才能将其可调度(running)设为0，从而不会发生上述行为。这一部分由`context_save` 完成。具体实现如下。
   ```C++
   static Context *kmt_context_save(Event ev, Context *context) {
       panic_on(!cur_task, "no valid task");
   
       cur_task->context = context; 
       // TODO: check last
       if (cur_last && cur_last != cur_task) {
           atomic_xchg(&cur_last->running, 0); // UNLOCK running
       }
       cur_last = cur_task;
       return NULL;
   }
   ```

   